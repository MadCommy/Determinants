\documentclass{beamer}
% \documentclass[handout]{beamer} 
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{centernot}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{eufrak}
\usepackage{blindtext}
\graphicspath{{pics/}}

\usetheme{Frankfurt}

\title{Computing the determinant}
\subtitle{Group 12}

\author{Catterwell, A. \quad Smith, M. \quad Wang, R. \quad Watson, K.}

\institute{University of Edinburgh}

\begin{document}

\begin{frame}
    \maketitle
\end{frame}

\begin{frame}{Overview}
    \tableofcontents
\end{frame}

\section{Why calculate determinants?}

\subsection{Invertibility of matrices}
\begin{frame}{Invertbililty of matrices}

    \begin{block}{Theorem}
        An $n \times n$ square matrix $A$ is invertible if and only if
        \[
            \det(A) \neq 0.
        \]
    \end{block}

\end{frame}

\subsection{Cramer's Rule}

\begin{frame}{Cramer's Rule}

    \begin{block}{Theorem}
        Given an equation $A\mathbf{x} = \mathbf{b}$
        The solutions for $\mathbf{x}$ are given by
        \[
            x_i = \frac{\det(A_i)}{\det(A)}
        \]
        with $A_i$ being the matrix formed by replacing the $i$th column
        of $A$ by $\mathbf{b}$.
    \end{block}

    % \pause{}

    It turns out this method has the same runtime complexity as Gaussian elimination for solving
    systems of linear equations.

\end{frame}

\subsection{Eigenvalues and Eigenvectors}

\begin{frame}{Eigenvalues and Eigenvectors}

    \begin{block}{Definition}
        The Eigenvalues $\lambda$ of a matrix $A$ are the roots of the characteristic polynomial
        as defined
        \[
            \chi_A = \det(A - \lambda I) = \mathbf{0}.
        \]
    \end{block}

    % \pause{}

    \begin{block}{Definition}
        The Eigenvectors of $A$ are the vectors $\mathbf{v}$ such that
        \[
            A\mathbf{v} = \lambda \mathbf{v}.
        \]
    \end{block}

\end{frame}

\begin{frame}{Eigenvalues and Eigenvectors (cont.)}

    The absolute value of the determinant of real vectors is equal to the volume
    of the parallelepiped spanned by those vectors.

    $f : \mathbb{R}^n \to \mathbb{R}^n$: the linear map represented by the $A$.
    $S$: any measurable subset of $\mathbb{R}^n$.
    \[
        \text{volume}(f(S)) = \sqrt{\det{({A^T}A)}} \times \text{volume}(S).
    \]
    The volume of any tetrahedron, given its vertices $a$, $b$, $c$, and $d$ is
    \[
        \frac{\det(a - b, b - c, c - d)}{6}.
    \]

\end{frame}

\subsection{Jacobian determinant}

\begin{frame}{Jacobian determinant}

    For $f : \mathbb{R}^n \to \mathbb{R}^n$,
    the Jacobian matrix is the $n \times n$ matrix whose entries are defined as
    \[
        D(f) = {\left( \frac{\partial f_i}{\partial x_j} \right)}_{1 \leq i, j \leq n.}
    \]
    Its determinant is known as the \emph{Jacobian determinant}.

    % \pause{}

    \begin{exampleblock}{}
        If the determinant of a continuously differentiable function $f$ at a point $p$ is\dots
        \begin{itemize}
            \item Non-zero, $f$ is invertible near a point $p$ in $\mathbb{R}^n$.
            \item Positive, then $f$ preserves orientation near $p$.
            \item Negative, then $f$ reverses orientation near $p$.
        \end{itemize}
    \end{exampleblock}

\end{frame}

\section{Algorithms for computing determinants}

\subsection{Leibniz formula}

\begin{frame}{Leibniz formula}

    \begin{block}{Definition}
        The Leibniz formula defines the determinant of $A \in \mathbb{M}(n)$ as
        \[
            \det(A) = \sum_{\sigma \in \mathfrak{S}_n}
            \left( \text{sgn}(\sigma) \cdot \prod_{i=1}^n a_{i,\sigma(i)} \right)
        \]
        where $\mathfrak{S}_n$ is the set of permutations length $n$.
    \end{block}

    \pause{}

    Computing the determinant using this method is slow with runtime $\mathcal{O}((n+1)!)$.

\end{frame}

\subsection{Laplace expansion}

\begin{frame}{Laplace expansion}

    The Laplace (1st row) expansion for computing determinants is usually the first method taught
    for computing determinants of $3 \times 3$ matrices and larger.

    \pause{}

    \begin{block}{Theorem}
        The formula for the (1st row) Laplace expansion of $A \in \mathbb{M}(n)$
        is given as:
        \[
            \det(A) = \sum_{j=1}^n a_{1,j}\, C_{1,j}
        \]
        where $C_{i,j} = {(-1)}^{i+j} \det{(A \langle i, j \rangle)}$ is the
        $(i, j)$ cofactor of $A$.
    \end{block}

    \pause{}

    Its runtime complexity of $\mathcal{O}(n!)$ is poor.

\end{frame}

\begin{frame}{Laplace expansion vs Leibniz formula}

    \begin{center}{}
        \includegraphics[height=180]{leibniz-laplace}
    \end{center}

    Runtimes are similar --- both run in exponential time.

\end{frame}

\subsection{LU decomposition}

\begin{frame}{What is LU decomposition?}

    \begin{block}{Definition}
        An LU decomposition of an invertible matrix $A$ is a factorization
        \[
            A = L U
        \]
        where $L$ and $U$ are lower and upper triangular matrices, respectively.
    \end{block}

\end{frame}

\begin{frame}{Is there always an LU decomposition?}

    \alert{No.}

    An LU decomposition of $A$ exists if and only if each of its \emph{leading principle minors}
    (contiguous square submatrices in the top-left corner of $A$),
    are also invertible.

    \pause{}

    \begin{exampleblock}{Example}
        \[
            \begin{pmatrix}{}
                0 & 1 \\
                1 & 0
            \end{pmatrix}
        \]
        This matrix is invertible but has no LU decomposition.
    \end{exampleblock}

    \pause{}

    \begin{alertblock}{}
        What can we do?
    \end{alertblock}

\end{frame}

\begin{frame}{PLU decomposition}

    \emph{Partial pivoting}.

    We can pivot the matrix into the correct form by multiplication with
    an orthogonal, permutation matrix $P$ (representing a permutation $\sigma_P$)
    which gives us the PLU decomposition:

    \[
        \sigma_P(A) = PA = LU
    \]

    \pause{}

    \begin{block}{}
        This technique works on \emph{any} matrix.
    \end{block}

\end{frame}

\begin{frame}{How it helps us compute determinants}

    Now that we have $PA = LU$, it follows that
    \begin{align*}
        A &= P^{-1}LU \\
          &= P^T LU
    \end{align*}
    since $P^{-1} = P^T$ by the definition of orthogonal matrices.

\end{frame}

\begin{frame}{How it helps us compute determinants (cont.)}

    Now that we have $A=P^T LU$, it follows that
    \begin{align*}
        \det(A)  & = \det(P^T L U) & \\
                 & = \det(P^T)\cdot \det(L)\cdot \det(U) & \text{(Thm. 4.4.1)} \\
                 & = \det(P)\cdot \det(L)\cdot \det(U)   & \text{(Lem. 4.4.4)}
    \end{align*}
    Given that
    \begin{itemize}
        \item the determinant of a triangular matrix is the product of its diagonal elements
        \item the determinant of a permutation matrix ($P$) is the parity of the permutation
            it represents ($\sigma_P$)
    \end{itemize}
    it follows that
    \begin{block}{}
        \[
            \det(A) = \text{sgn}(\sigma_P) \cdot \left( \prod_{i=1}^{n} l_{i,i} \right)
            \left( \prod_{i=1}^{n} u_{i,i} \right).
        \]
    \end{block}

\end{frame}

\begin{frame}{How do we find the PLU decomposition?}

    \begin{center}{}
        \includegraphics[height=140]{PLUcode.png}
    \end{center}

    \begin{alertblock}{}
        This algorithm only works on invertible matrices (line 8 division).
    \end{alertblock}

\end{frame}

\begin{frame}{How do we find the PLU decomposition? (cont.)}
    \begin{exampleblock}{Example}
        \[
            \begin{pmatrix}{}
                0 & 1 \\
                1 & 0
            \end{pmatrix}
        \]
        \pause{}
        \[
            \begin{pmatrix}{}
                0 & 1 \\
                1 & 0
            \end{pmatrix}
        \]
        \pause{}
        \[
            \begin{pmatrix}{}
                0 & 1 \\
                1 & 0
            \end{pmatrix}
        \]
        \pause{}
        \[
            \begin{pmatrix}{}
                0 & 1 \\
                1 & 0
            \end{pmatrix}
        \]

    \end{exampleblock}

\end{frame}

\begin{frame}{Runtime analysis}
    How quick is it?
    \begin{itemize}
        \item The PLU decomposition can be computed in $\mathcal{O}(n^3)$ time.
        \item The determinants of the triangular matrices computed in $\mathcal{O}(n)$ time.
        \item The parity of the permutation matrix in $\mathcal{O}(n^2)$ time.
    \end{itemize}

    Therefore the total runtime for computing the determinant using the method is
    \[
        \mathcal{O}(n^3) + \mathcal{O}(n^2) + \mathcal{O}(n) = \mathcal{O}(n^3).
    \]

\end{frame}


\begin{frame}{Laplace expansion vs LU decomposition}

    \begin{center}{}
        \includegraphics[height=180]{laplace-lu}
    \end{center}

    The difference between the exponential and polynomial-time function is clear.
    % Let's look at a better algorithm.
\end{frame}


\begin{frame}{Limitations of LU decomposition}

    The main problem with the LU decomposition algorithm used is that it often requires division.

    \begin{itemize}
        \pause{}
        \item Division is not a ring operation, so it won't work on matrices over rings.
        \pause{}
        \item Unless we compute exactly (difficult on a computer), precision may be lost.
    \end{itemize}

    \pause{}
    Let's try something else\dots

\end{frame}


\subsection{Gaussian elimination}

\begin{frame}{Gaussian elimination}

    \begin{itemize}

        \item The determinant of a triangular matrix can be computed by taking the product of its
            diagonal entries (which is a quick $\mathcal{O}(n)$ operation).

        \item Any invertible square matrix can be transformed into echelon form by performing
            Gaussian elimination, which takes $\mathcal{O}(n^3)$ time.

        \item Quite similar to algorithm for LU decomposition.

    \end{itemize}

    \pause{}

    So how does it compare to LU decomposition?

\end{frame}

\begin{frame}{Gaussian elimination vs LU decomposition}

    \begin{center}{}
        \includegraphics[height=180]{lu-gauss}
    \end{center}

    The difference in runtimes is small (a constant factor).

\end{frame}

\begin{frame}{Gaussian elimination (cont.)}

    Conventional Gaussian elimination (like LU decomposition) requires division.

    \pause{}

    This is can be addressed by using\dots

\end{frame}

\subsection{Bareiss algorithm}

\begin{frame}{Bareiss algorithm}

    \begin{itemize}

        \item Addresses the issue of precision-loss by performing \emph{integer-preserving}
            Gaussian elimination on integer matrices.

        \item The runtime complexity is $\mathcal{O}(n^3)$ which is the same as conventional
            Gaussian Elimination, whilst preserving exactness.

    \end{itemize}

    \pause{}

    Unfortunately, the maths behind this is a bit hard\dots

\end{frame}

\subsection{Bird's algorithm}

\begin{frame}{Bird's algorithm}

    Define $\mu : \mathbb{M}(n) \to \mathbb{M}(n)$:
    \[
        \mu(X) =
        \begin{pmatrix}{}
            \mu_{2,2} - x_{2,2} & x_{1,2}             & \cdots & x_{1,n-1}           & x_{1,n} \\
            0                   & \mu_{3,3} - x_{3,3} & \cdots & x_{2,n-1}           & x_{2,n} \\
            \vdots              & \vdots              & \ddots & \vdots              & \vdots \\
            0                   & 0                   & \cdots & \mu_{n,n} - x_{n,n} & x_{n-1,n} \\
            0                   & 0                   & \cdots & 0                   & 0
        \end{pmatrix}
    \]

    \pause{}

    and $F_A : \mathbb{M}(n) \to \mathbb{M}(n)$,
    with $A \in \mathbb{M}(n)$
    \begin{align*}{}
        F_A(X)    & = \mu(X)\cdot A \\
        F_A^2(X)  & = \mu(F_A(X)) \cdot A \\
                  & \vdots \\
        F_A^n(X)  & = \mu(F_A^{n-1}(X)) \cdot A. \\
    \end{align*}

\end{frame}

\begin{frame}{Bird's algorithm (cont.)}

    \begin{block}{Bird's Theorem}
        \[
            F_A^{n-1}(A) =
            \begin{pmatrix}{}
                d      & 0      & \cdots & 0 \\
                0      & 0      & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0      & 0      & 0      & 0
            \end{pmatrix}
            \text{with} \ d =
            \begin{cases}{}
                \det(A)  & \text{odd} \ n \\
                -\det(A) & \text{even} \ n. \\
            \end{cases}
        \]
    \end{block}

    \pause{}

    \begin{itemize}

        \item Enables the \emph{division-free} computation of determinants in
            $\mathcal{O}(n\cdot M(n))$ where $M(n)$ is the runtime complexity of the matrix
            multiplication algorithm used.

        \item If the conventional $\mathcal{O}(n^3)$ matrix multiplication algorithm is used,
            then Bird's algorithm will run in $\mathcal{O}(n^4)$ time.

        \item But this can be reduced to $\mathcal{O}(n^{3.8})$ by using a faster
            (e.g. \emph{Strassen}) algorithm for matrix multiplication.

    \end{itemize}

\end{frame}

\begin{frame}{Bird's algorithm vs LU decomposition}

    \begin{center}{}
        \includegraphics[height=180]{bird-lu}
    \end{center}

    Bird's runtimes increase noticeably more rapidly than LU decomposition,
    but it's still polynomial.

\end{frame}

\section{Epilogue}

\subsection{Summary of determinant algorithms}

\begin{frame}{Summary of determinant algorithms}

    \begin{center}
        \begin{tabular}{l l l}
            \toprule
            \emph{Algorithm}     & \emph{Runtime}           & \emph{Exact?} \\
            \midrule
            Leibniz formula      & $\mathcal{O}((n+1)!)$    & Yes \\
            Laplace expansion    & $\mathcal{O}(n!)$        & Yes \\
            LU decomposition     & $\mathcal{O}(n^3)$       & No \\
            Gaussian elimination & $\mathcal{O}(n^3)$       & No \\
            Bareiss algorithm    & $\mathcal{O}(n^3)$       & Yes \\
            Bird's algorithm     & $\mathcal{O}(n^{3.8})$   & Yes \\
            \bottomrule
        \end{tabular}
    \end{center}

\end{frame}

\subsection{How fast is Maple's implementation?}

\begin{frame}{How fast is Maple's built-in determinant function?}

    \begin{center}{}
        \includegraphics[height=180]{gauss-maple}
    \end{center}

    Very. Maple's optimisation means a fair comparison cannot be made.

\end{frame}

\begin{frame}

    \begin{center}
        \Huge{Thanks!}
    \end{center}

\end{frame}

\end{document}
